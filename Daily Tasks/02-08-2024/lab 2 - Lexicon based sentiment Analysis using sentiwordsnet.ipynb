{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2 : Sentiment Analysis using SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis\n",
    "Sentiment analysis, the process of determining the emotional tone behind words, can be approached using various techniques. Here are three primary categories: lexicon-based, machine learning-based, and deep learning-based techniques.\n",
    "\n",
    "**SentiWordNet:** Each word is annotated with positive, negative, and objective scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words in a language (like \"the\", \"is\", \"in\", etc.) that are often removed from text data because they don't carry significant meaning or contribute much to the analysis. Removing stopwords is a common preprocessing step in natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bhawa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The movie was fantastic! I really enjoyed it.\n",
      "SentiWordNet Sentiment Score: 1.5\n"
     ]
    }
   ],
   "source": [
    "# Function to map NLTK POS tags to WordNet POS tags\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    if tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    if tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    return None\n",
    "\n",
    "# Function to get sentiment scores for a text\n",
    "def get_sentiment_scores(text):\n",
    "    sentiment = 0.0\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for word, tag in tagged:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag:\n",
    "            synsets = wn.synsets(word, pos=wn_tag)\n",
    "            if synsets:\n",
    "                swn_synset = swn.senti_synset(synsets[0].name())\n",
    "                sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "    return sentiment\n",
    "\n",
    "# Sample text for sentiment analysis\n",
    "text = \"The movie was fantastic! I really enjoyed it.\"\n",
    "\n",
    "# Get sentiment score\n",
    "sentiment_score = get_sentiment_scores(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"SentiWordNet Sentiment Score: {sentiment_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The movie was fantastic! I really enjoyed it.\"\n",
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'movie', 'was', 'fantastic', '!', 'I', 'really', 'enjoyed', 'it', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('movie', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('fantastic', 'JJ'),\n",
       " ('!', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('really', 'RB'),\n",
       " ('enjoyed', 'VBD'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    if tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    if tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, tag in tagged:\n",
    "    if tag.startswith('B-'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('movie.n.01')]\n",
      "[Synset('be.v.01'), Synset('be.v.02'), Synset('be.v.03'), Synset('exist.v.01'), Synset('be.v.05'), Synset('equal.v.01'), Synset('constitute.v.01'), Synset('be.v.08'), Synset('embody.v.02'), Synset('be.v.10'), Synset('be.v.11'), Synset('be.v.12'), Synset('cost.v.01')]\n",
      "[Synset('antic.s.01'), Synset('fantastic.s.02'), Synset('fantastic.s.03'), Synset('fantastic.s.04'), Synset('fantastic.s.05')]\n",
      "[Synset('truly.r.01'), Synset('actually.r.01'), Synset('in_truth.r.01'), Synset('very.r.01')]\n",
      "[Synset('enjoy.v.01'), Synset('enjoy.v.02'), Synset('love.v.02'), Synset('enjoy.v.04'), Synset('delight.v.02')]\n"
     ]
    }
   ],
   "source": [
    "for word, tag in tagged:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag:\n",
    "            synsets = wn.synsets(word, pos=wn_tag)\n",
    "            print(synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'eng'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Load all synsets with a given lemma and part of speech tag.\n",
      "If no pos is specified, all synsets for all parts of speech\n",
      "will be loaded.\n",
      "If lang is specified, all the synsets associated with the lemma name\n",
      "of that language will be returned.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\bhawa\\miniconda3\\envs\\datascience\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\n",
      "\u001b[1;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "wn.synsets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 'JJ'), ('honest', 'NN')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"good\", \"honest\"]\n",
    "tagged=nltk.pos_tag(words)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_tags=[]\n",
    "for word, tag in tagged:\n",
    "    wn_tags.append(penn_to_wn(tag))\n",
    "wn_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('good.a.01'),\n",
       " Synset('full.s.06'),\n",
       " Synset('good.a.03'),\n",
       " Synset('estimable.s.02'),\n",
       " Synset('beneficial.s.01'),\n",
       " Synset('good.s.06'),\n",
       " Synset('good.s.07'),\n",
       " Synset('adept.s.01'),\n",
       " Synset('good.s.09'),\n",
       " Synset('dear.s.02'),\n",
       " Synset('dependable.s.04'),\n",
       " Synset('good.s.12'),\n",
       " Synset('good.s.13'),\n",
       " Synset('effective.s.04'),\n",
       " Synset('good.s.15'),\n",
       " Synset('good.s.16'),\n",
       " Synset('good.s.17'),\n",
       " Synset('good.s.18'),\n",
       " Synset('good.s.19'),\n",
       " Synset('good.s.20'),\n",
       " Synset('good.s.21')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"good\", pos=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

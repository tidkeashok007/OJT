{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example true and predicted labels for binary classification\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "print(\"Accuracy:\", accuracy(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "def precision(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "print(\"Precision:\", precision(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Recall (Sensitivity)\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "print(\"Recall:\", recall(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# F1 Score\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2 3]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Confusion Matrix\n",
    "def display_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "display_confusion_matrix(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true = [75 80 93 19 19 49  6 87 55 49]\n",
      "y_pred = [88 19 46 14 89 98 28 61 60 56]\n"
     ]
    }
   ],
   "source": [
    "# For regression\n",
    "y_true = np.random.randint(1,100,(10))\n",
    "y_pred = np.random.randint(1,100,(10))\n",
    "print(f\"y_true = {y_true}\")\n",
    "print(f\"y_pred = {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 30.5\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1465.9\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error (MSE)\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 38.28707353663897\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (MAPE): 105.87861573617529\n"
     ]
    }
   ],
   "source": [
    "# Mean Absolute Percentage Error (MAPE)\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error (MAPE):\", mape(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above can be done using scikit-learn library also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Example true and predicted labels for binary classification\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, root_mean_squared_error\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Data for regression\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m,(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m,(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# for regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, root_mean_squared_error\n",
    "# Data for regression\n",
    "y_true = np.random.randint(1,100,(10))\n",
    "y_pred = np.random.randint(1,100,(10))\n",
    "print(f\"y_true = {y_true}\")\n",
    "print(f\"y_pred = {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean absolute Error is : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmean_absolute_error\u001b[49m(y_true,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean absolute Percentage Error is : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_absolute_percentage_error(y_true,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Squared Error is : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_squared_error(y_true,\u001b[38;5;250m \u001b[39my_pred)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_absolute_error' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute Error is : {mean_absolute_error(y_true, y_pred)}\")\n",
    "print(f\"Mean absolute Percentage Error is : {mean_absolute_percentage_error(y_true, y_pred)}\")\n",
    "print(f\"Mean Squared Error is : {mean_squared_error(y_true, y_pred)}\")\n",
    "print(f\"Root mean squared Error is : {mean_absolute_percentage_error(y_true, y_pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

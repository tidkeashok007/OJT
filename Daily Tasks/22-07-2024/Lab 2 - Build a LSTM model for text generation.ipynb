{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 2 - Build a LSTM model for text generation\n",
    "#### Basic Steps for LSTM Text Generation\n",
    "##### 1. Load and Preprocess Data:\n",
    "- Load the text data from a source.\n",
    "- Convert the text to lowercase to ensure consistency.\n",
    "\n",
    "##### 2. Create Character Mappings:\n",
    "- Identify the unique characters in the text.\n",
    "- Create dictionaries to map characters to indices and vice versa.\n",
    "\n",
    "##### 3. Create Input Sequences:\n",
    "- Define the length of input sequences (maxlen).\n",
    "- Extract overlapping sequences of text (sentences) and their corresponding next characters (next_chars).\n",
    "\n",
    "##### 4. Vectorize the Data:\n",
    "- Convert the sequences and the next characters to one-hot encoded vectors (x for input sequences, y for next characters).\n",
    "\n",
    "##### 5. Build the LSTM Model:\n",
    "- Define a sequential model.\n",
    "- Add LSTM layers and a Dropout layer for regularization.\n",
    "- Add a Dense layer with a softmax activation function to predict the next character.\n",
    "\n",
    "##### 6. Compile the Model:\n",
    "- Compile the model using an optimizer (e.g., Adam) and a loss function (e.g., categorical cross-entropy).\n",
    "\n",
    "##### 7. Train the Model:\n",
    "- Train the model on the vectorized data using a specified batch size and number of epochs.\n",
    "- Optionally, use callbacks to monitor training progress and generate text at intervals.\n",
    "\n",
    "##### 8. Generate Text:\n",
    "- Define a function to sample the next character based on model predictions and a temperature parameter.\n",
    "- Generate text by predicting the next character iteratively, starting from a random seed sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:24.872339Z",
     "iopub.status.busy": "2024-05-21T05:07:24.871876Z",
     "iopub.status.idle": "2024-05-21T05:07:24.877793Z",
     "shell.execute_reply": "2024-05-21T05:07:24.876867Z",
     "shell.execute_reply.started": "2024-05-21T05:07:24.872309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:24.879743Z",
     "iopub.status.busy": "2024-05-21T05:07:24.879339Z",
     "iopub.status.idle": "2024-05-21T05:07:24.895252Z",
     "shell.execute_reply": "2024-05-21T05:07:24.894437Z",
     "shell.execute_reply.started": "2024-05-21T05:07:24.879711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found, using CPU\n"
     ]
    }
   ],
   "source": [
    "# This cell of the code is useful if you have a GPU like Nvidia or If you are using a cloud platform like kaggle\n",
    "# Ensure GPU is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:24.896492Z",
     "iopub.status.busy": "2024-05-21T05:07:24.896234Z",
     "iopub.status.idle": "2024-05-21T05:07:24.913623Z",
     "shell.execute_reply": "2024-05-21T05:07:24.912760Z",
     "shell.execute_reply.started": "2024-05-21T05:07:24.896470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the text data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.lower()  # Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:24.916392Z",
     "iopub.status.busy": "2024-05-21T05:07:24.916047Z",
     "iopub.status.idle": "2024-05-21T05:07:24.931567Z",
     "shell.execute_reply": "2024-05-21T05:07:24.930680Z",
     "shell.execute_reply.started": "2024-05-21T05:07:24.916362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create character-level mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_char = {i: char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:24.932794Z",
     "iopub.status.busy": "2024-05-21T05:07:24.932558Z",
     "iopub.status.idle": "2024-05-21T05:07:25.341906Z",
     "shell.execute_reply": "2024-05-21T05:07:25.341124Z",
     "shell.execute_reply.started": "2024-05-21T05:07:24.932774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cut the text into sequences\n",
    "maxlen = 40  # Length of input sequences\n",
    "step = 1     # Step size to create sequences\n",
    "sentences = []  # Input sequences\n",
    "next_chars = []  # Output characters\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:25.343177Z",
     "iopub.status.busy": "2024-05-21T05:07:25.342925Z",
     "iopub.status.idle": "2024-05-21T05:07:38.412979Z",
     "shell.execute_reply": "2024-05-21T05:07:38.411999Z",
     "shell.execute_reply.started": "2024-05-21T05:07:25.343154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:38.414464Z",
     "iopub.status.busy": "2024-05-21T05:07:38.414172Z",
     "iopub.status.idle": "2024-05-21T05:07:38.504765Z",
     "shell.execute_reply": "2024-05-21T05:07:38.504068Z",
     "shell.execute_reply.started": "2024-05-21T05:07:38.414440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhawa\\miniconda3\\envs\\dataScience\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:38.506077Z",
     "iopub.status.busy": "2024-05-21T05:07:38.505778Z",
     "iopub.status.idle": "2024-05-21T05:07:38.511057Z",
     "shell.execute_reply": "2024-05-21T05:07:38.510203Z",
     "shell.execute_reply.started": "2024-05-21T05:07:38.506054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:38.512587Z",
     "iopub.status.busy": "2024-05-21T05:07:38.512267Z",
     "iopub.status.idle": "2024-05-21T05:07:38.523739Z",
     "shell.execute_reply": "2024-05-21T05:07:38.523062Z",
     "shell.execute_reply.started": "2024-05-21T05:07:38.512555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to generate text in each epoch\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    for i in generated:\n",
    "        time.sleep(0.05)\n",
    "        print(i,end=\"\", flush=True)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        print(next_char,end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Callback to generate text after each epoch\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:07:38.527254Z",
     "iopub.status.busy": "2024-05-21T05:07:38.526763Z",
     "iopub.status.idle": "2024-05-21T05:22:02.616032Z",
     "shell.execute_reply": "2024-05-21T05:22:02.615124Z",
     "shell.execute_reply.started": "2024-05-21T05:07:38.527229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T05:22:02.617463Z",
     "iopub.status.busy": "2024-05-21T05:22:02.617162Z",
     "iopub.status.idle": "2024-05-21T05:22:02.663147Z",
     "shell.execute_reply": "2024-05-21T05:22:02.662477Z",
     "shell.execute_reply.started": "2024-05-21T05:22:02.617437Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"LSTM for Text generation 90epocs 3 LSTM layers.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'conv_utils' from 'keras.utils' (c:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKERAS_BACKEND\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaidml.keras.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplaidml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mplaidml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Ensure GPU is being used\u001b[39;00m\n\u001b[0;32m     18\u001b[0m physical_devices \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\plaidml\\keras\\__init__.py:68\u001b[0m, in \u001b[0;36minstall_backend\u001b[1;34m(import_path, backend, trace_file)\u001b[0m\n\u001b[0;32m     65\u001b[0m sys\u001b[38;5;241m.\u001b[39mmeta_path \u001b[38;5;241m=\u001b[39m [_PlaidMLBackendFinder(import_path, backend, trace_file)] \u001b[38;5;241m+\u001b[39m sys\u001b[38;5;241m.\u001b[39mmeta_path\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Hack around Keras expecting everything not Tensorflow to be Theano.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m conv_utils\n\u001b[0;32m     69\u001b[0m conv_utils\u001b[38;5;241m.\u001b[39mconvert_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'conv_utils' from 'keras.utils' (c:\\Users\\bhawa\\miniconda3\\envs\\tester\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Setup PlaidML\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "\n",
    "# Ensure GPU is being used\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "# Load and preprocess the text data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', 'https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.lower()  # Convert to lowercase\n",
    "\n",
    "# Create character-level mappings\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = {char: i for i, char in enumerate(chars)}\n",
    "indices_char = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "# Cut the text into sequences\n",
    "maxlen = 40  # Length of input sequences\n",
    "step = 1     # Step size to create sequences\n",
    "sentences = []  # Input sequences\n",
    "next_chars = []  # Output characters\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "# Vectorization\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool_)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential([\n",
    "    LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dense(len(chars), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Function to sample the next character given the model's predictions\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# Function to generate text in each epoch\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    for i in generated:\n",
    "        time.sleep(0.05)\n",
    "        print(i, end=\"\", flush=True)\n",
    "\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "        print(next_char, end=\"\", flush=True)\n",
    "    print()\n",
    "\n",
    "# Callback to generate text after each epoch\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
